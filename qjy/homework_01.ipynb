{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一次平时作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "### (a) 数据规范化\n",
    "某数据集记录了某单位成年人的身高和体重数据。身高范围为 1.4-1.9 m，体重范围为 40-90kg。请结合表1，回答下列问题： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 表1. 体型数据表\n",
    "\n",
    "| ID | 身高 (m) | 体重 (kg) |\n",
    "|----|---------|----------|\n",
    "| 1  | 1.70    | 50       |\n",
    "| 2  | 1.60    | 50       |\n",
    "| 3  | 1.70    | 60       |\n",
    "| 4  | 1.65    | 45       |\n",
    "| ...| ...     | ...      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （1）请以“判断用户体型相似性”为例，说明数据规范化的必要性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在“判断用户体型相似性”的场景中，数据规范化的必要性可以从以下几个方面来理解：\n",
    "\n",
    "- **尺度一致性**：身高和体重的量级和单位不同。身高通常在一米左右，而体重则是几十千克。如果直接使用这些原始数据进行相似性比较，例如通过计算欧氏距离，体重因其较大的数值将对结果产生不成比例的影响。规范化处理可以确保每个特征对相似性度量的贡献是均衡的。\n",
    "\n",
    "- **提升算法性能**：许多机器学习算法在处理数据时，如果数据特征处于相同的尺度，算法会表现得更好。比如，在使用基于距离的算法（如K-最近邻）时，未规范化的数据可能导致算法过分重视某些特征。\n",
    "\n",
    "- **加快收敛速度**：对于需要迭代求解的算法（如梯度下降），规范化的数据可以加快收敛速度，因为每个特征值对参数更新的影响大致相同。\n",
    "\n",
    "- **消除单位影响**：在比较不同单位或量级的特征时，单位和量级的差异可能会影响分析的结果。规范化可以消除这些因素的影响，使得特征之间的比较更加公平。\n",
    "\n",
    "- **增强解释性**：规范化后的数据在相同的尺度上，可以使得特征之间的比较更加直观，有助于发现数据间的真实关系。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）请采用 Min-Max 规范化，将表 1 用户的身高和体重数据规范化到[0,1]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性函数规范化（Min-Max Scaling）。它对原始数据进行线性变换，使结果映射到[0, 1]的范围，实现对原始数据的等比缩放。归一化公式如下：\n",
    "\n",
    "$$\n",
    "X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}}\n",
    "$$\n",
    "\n",
    "其中$X$为原数据(raw data)，$X_norm$为规范化后数据(normalized data)\n",
    "\n",
    "本题中，身高最大值为1.7m，最小值为1.6m；本题中，身高最大值为60kg，最小值为45kg。\n",
    "\n",
    "式子就转变为\n",
    "\n",
    "$$\n",
    "height_{norm, i} =  \\frac{height_{i} - 1.6}{1.7-1.6}\n",
    "$$\n",
    "\n",
    "$$\n",
    "weight_{norm, i} =  \\frac{weight_{i} - 45}{60-45}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\forall i \\in ID\n",
    "$$\n",
    "\n",
    "分别将其带入得到:\n",
    "\n",
    "| ID | 身高_normalized | 体重_normalized |\n",
    "|----|-----------------|-------------|\n",
    "| 1  | 1.00            | 0.33        |\n",
    "| 2  | 0.00            | 0.33        |\n",
    "| 3  | 1.00            | 1.00        |\n",
    "| 4  | 0.50            | 0.00        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （3）用户 2、3、4 中，谁和用户 1 的体型更相似？请给出结论和计算依据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据补充知识可得，我们可以根据欧式距离判断相似性。\n",
    "\n",
    "因此我们来判断各个非1的ID与ID为1的距离：\n",
    "\n",
    "$$\n",
    "D(i,1) = \\sqrt{(height_{norm,i} - 1)^2 + (weight_{norm,i} - 0.33)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\forall i \\in ID\n",
    "$$\n",
    "\n",
    "$$\n",
    "i \\neq 1\n",
    "$$\n",
    "\n",
    "结果得到：\n",
    "\n",
    "| ID | 欧氏距离   |\n",
    "|----|----------|\n",
    "| 2  | 1.000000 |\n",
    "| 3  | 0.666667 |\n",
    "| 4  | 0.600925 |\n",
    "\n",
    "因此ID为4的与1最为相似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) 数据离散化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 表2. 个人信息表\n",
    "\n",
    "| 年龄(岁) | 性别 | 年收入(万元) |\n",
    "|---------|-----|------------|\n",
    "| 25      | 女  | 10         |\n",
    "| 27      | 男  | 25         |\n",
    "| 30      | 女  | 30         |\n",
    "| 45      | 男  | 60         |\n",
    "| 28      | 女  | 40         |\n",
    "| 32      | 女  | 20         |\n",
    "| 52      | 女  | 50         |\n",
    "| 35      | 男  | 30         |\n",
    "| 55      | 女  | 100        |\n",
    "| 48      | 男  | 120        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别利用等宽离散化和等深离散化将表 2 中的属性“年收入(万元)”转换为“低收入”、“中收入”和“高收入”三档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**等宽离散化**是一种将连续型变量分成若干个等宽的区间的方法。 其原理是先确定要分成的区间数量，然后根据数据的最小值和最大值计算出每个区间的宽度，最后将数据映射到对应的区间中](https://zhuanlan.zhihu.com/p/655381873)。\n",
    "\n",
    "因此我们需要有三个区间：“低收入”、“中收入”和“高收入”。\n",
    "\n",
    "每个区间的长度为\n",
    "\n",
    "$$\n",
    "\\frac{120-10}{3} = 36.67\n",
    "$$\n",
    "\n",
    "因此，\n",
    "\n",
    " - 低收入对应 10 - 46.67 万元\n",
    " - 中收入对应 46.67 - 83.34 万元\n",
    " - 高收入对应 83.34 - 120 万元\n",
    "\n",
    " 最后得到：\n",
    "\n",
    "| 年收入(万元) | 年收入_等宽 |\n",
    "|-------------|------------|\n",
    "| 10          | 低收入       |\n",
    "| 25          | 低收入       |\n",
    "| 30          | 低收入       |\n",
    "| 60          | 中收入       |\n",
    "| 40          | 低收入       |\n",
    "| 20          | 低收入       |\n",
    "| 50          | 中收入       |\n",
    "| 30          | 低收入       |\n",
    "| 100         | 高收入       |\n",
    "| 120         | 高收入       |\n",
    "\n",
    "[**等深离散化**实际上是等频离散化(Equal-Frequency Discretization)，将特征值的累积频率分布均匀划分成若干个区间。](https://open.passingai.com/content/703.html)\n",
    "\n",
    "在等深离散化的过程中，数据被分为三个类别，具体的分类规则如下：\n",
    "\n",
    "- **低收入**：包含最低的3-4个数据点。\n",
    "- **中收入**：紧接着的3-4个数据点。\n",
    "- **高收入**：剩余的数据点。\n",
    "\n",
    "这种分类方法旨在确保每个类别中包含大致相等数量的数据点，从而反映出数据的分布特性。\n",
    "\n",
    "\n",
    "我们有 `n=10` 个数据点，并希望计算 `1/3` 和 `2/3` 的分位点位置。计算分位点位置的公式为：\n",
    "\n",
    "$$\n",
    "position = q * (n + 1)\n",
    "$$\n",
    "\n",
    "其中'q'是所求分位数(`1/3` 或 `2/3`)\n",
    "\n",
    "- 对于 `1/3` 的分位点位置：\n",
    "\n",
    "$$\n",
    "position_{\\frac{1}{3}} = \\frac{1}{3} × (10 + 1) = 3.67\n",
    "$$\n",
    "\n",
    "- 对于 `2/3` 的分位点位置：\n",
    "\n",
    "$$\n",
    "position_{\\frac{2}{3}} = \\frac{2}{3} × (10 + 1) = 7.33\n",
    "$$\n",
    "\n",
    "由于分位点位置不是整数，我们需要在相邻的数据点之间进行插值来计算分位点的实际值\n",
    "\n",
    "对于 `1/3` 的分位点值，位置大约在 3 和 4 之间，所以我们在第 3 个（30万元）和第 4 个数据点（40万元）之间插值：\n",
    "\n",
    "$$\n",
    "position_{\\frac{1}{3}} = 30 + (40-30)*(3.67-3) = 28.33\n",
    "$$\n",
    "\n",
    "对于 `2/3` 的分位点值，位置大约在 7 和 8 之间，所以我们在第 7 个（50万元）和第 8 个数据点（60万元）之间插值：\n",
    "\n",
    "$$\n",
    "position_{\\frac{2}{3}} = 50 + (60-50)*(7.33-7) = 53.33\n",
    "$$\n",
    "\n",
    "结合之前规则，\n",
    "\n",
    "| 年收入(万元) | 年收入_等深 |\n",
    "|-------------|------------|\n",
    "| 10          | 低收入       |\n",
    "| 25          | 低收入       |\n",
    "| 30          | 中收入       |\n",
    "| 60          | 高收入       |\n",
    "| 40          | 中收入       |\n",
    "| 20          | 低收入       |\n",
    "| 50          | 中收入       |\n",
    "| 30          | 低收入       |\n",
    "| 100         | 高收入       |\n",
    "| 120         | 高收入       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 Titanic 数据集（详见 train.csv 和 test.csv），预测乘客及船员的生存情况。通过数据预处理，开展分类分析，主要进行数据集划分、模型参数选择和实验结果比较，重点实践决策树、逻辑回归和朴素贝叶斯三个算法。作业要求提交完整的分类分析报告，包括文字叙述、代码和运行结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们按照要求先读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "df_train = pd.read_excel('train.xlsx')\n",
    "df_test = pd.read_excel('test.xlsx')\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标签选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来需要选择特征和标签，观察column可知，目标变量为\"Survived\"，我们需要决定哪些特征对于预测目标是有意义的，Pclass代表几等仓，SibSp 代表 Siblings/Spouses aboard；船上的兄弟姐妹/配偶， parch 代表 Parents/Children aboard；船上的父母儿童，fare表示票价，cabin代表船舱号，embarked代表上船的不同的港口。选择以下特征进行分析：\n",
    "\n",
    "- **Pclass（乘客等级）**：这是一个重要的特征，因为不同的乘客等级可能会影响生存几率。\n",
    "- **Sex（性别）**：性别是影响生存几率的一个显著因素。\n",
    "- **Age（年龄）**：年龄可能会影响生存几率，儿童和老人可能有不同的生存几率。\n",
    "- **SibSp（兄弟姐妹/配偶数量）**：表示乘客在船上的兄弟姐妹和配偶的数量，泰坦尼克号里有一幕是一些绅士决定让妇女儿童先走，但是他们妻子选择留下来和他们一起，这可能影响生存几率。\n",
    "- **Parch（父母/子女数量）**：表示乘客在船上的父母和子女的数量，这同样可能影响生存几率。\n",
    "- **Fare（票价）**：票价可能反映乘客的经济状况和乘客等级，进而影响生存几率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Pclass', 'Sex', 'Age','SibSp', 'Parch', 'Fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理缺失值以及特征编码和归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "no_value_train = df_train.isnull().sum()\n",
    "print(no_value_train)\n",
    "\n",
    "no_value_test = df_train.isnull().sum()\n",
    "print(no_value_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由以上代码我们可以判断每一列的空值情况，对于年龄我们直接利用前后的中位数进行估计。对于类别特征我们用[独热编码](https://zhuanlan.zhihu.com/p/134495345)进行处理，我们可以构建一个预处理器并应用到训练模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38623105, -0.49947002, -0.4002477 , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 1.37137004,  0.61699237, -0.4002477 , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 2.55353683, -0.49947002, -0.4002477 , ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.70147553, -0.49947002, -0.4002477 , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.20485235, -0.49947002, -0.4002477 , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.20485235,  0.61699237,  0.61989583, ...,  0.        ,\n",
       "         0.        ,  1.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),  # 填充缺失值\n",
    "            ('scaler', StandardScaler())  # 归一化\n",
    "        ]), ['Age', 'SibSp', 'Parch']),  # 数值型特征\n",
    "        ('cat', OneHotEncoder(), ['Sex', 'Pclass'])  # 类别型特征\n",
    "    ])\n",
    "\n",
    "features = ['Age', 'SibSp', 'Parch', 'Sex', 'Pclass']\n",
    "\n",
    "X_train = df_train[features]\n",
    "y_train = df_train['Survived']\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test['Survived']\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.fit_transform(X_test)\n",
    "\n",
    "# X_train_transformed\n",
    "X_test_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据集划分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们直接由两个文件test.xlsx和train.xlsx，因此无需进行数据集划分 (如果没有test这个数据集我们将数据集划分为训练集和测试集，其中80%的数据用于训练，20%的数据用于测试。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train_transformed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树，逻辑回归和朴素贝叶斯实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用默认参数训练了三类模型，并在测试集上进行了评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树模型准确率: 0.6555023923444976\n",
      "逻辑回归模型准确率: 0.7535885167464115\n",
      "朴素贝叶斯模型准确率: 0.7799043062200957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# dt = decision tree\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# lr = logic regression\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# nb = naive Bayes\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# training\n",
    "dt_classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "lr_classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "nb_classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# prediction\n",
    "dt_pred = dt_classifier.predict(X_test_transformed)\n",
    "lr_pred = lr_classifier.predict(X_test_transformed)\n",
    "nb_pred = nb_classifier.predict(X_test_transformed)\n",
    "\n",
    "# calculate the accuracy\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
    "\n",
    "print(\"决策树模型准确率:\", dt_accuracy)\n",
    "print(\"逻辑回归模型准确率:\", lr_accuracy)\n",
    "print(\"朴素贝叶斯模型准确率:\", nb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树模型交叉验证准确率: [0.74301676 0.79775281 0.79775281 0.79213483 0.83146067]\n",
      "平均准确率: 0.7924235766744083\n",
      "逻辑回归模型交叉验证准确率: [0.7877095  0.78651685 0.79213483 0.76404494 0.83707865]\n",
      "平均准确率: 0.793496955621116\n",
      "朴素贝叶斯模型交叉验证准确率: [0.65921788 0.69101124 0.6741573  0.66292135 0.65730337]\n",
      "平均准确率: 0.6689222271043876\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# dt_scores = cross_val_score(dt_classifier, X_train_transformed, y, cv=5, scoring='accuracy')\n",
    "# print(\"决策树模型交叉验证准确率:\", dt_scores)\n",
    "# print(\"平均准确率:\", dt_scores.mean())\n",
    "\n",
    "# lr_scores = cross_val_score(lr_classifier, X_train_transformed, y, cv=5, scoring='accuracy')\n",
    "# print(\"逻辑回归模型交叉验证准确率:\", lr_scores)\n",
    "# print(\"平均准确率:\", lr_scores.mean())\n",
    "\n",
    "# nb_scores = cross_val_score(nb_classifier, X_train_transformed, y, cv=5, scoring='accuracy')\n",
    "# print(\"朴素贝叶斯模型交叉验证准确率:\", nb_scores)\n",
    "# print(\"平均准确率:\", nb_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们比较准确率（accuracy）,而不是 R2 误差。准确率是指模型正确分类的样本数量与总样本数量之比。在二分类问题中，准确率可以简单地表示为模型正确预测的正例和负例的比例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是利用泰坦尼克号的船员数据特征 `features = ['Pclass', 'Sex', 'Age','SibSp', 'Parch', 'Fare']` 预测存活情况 `Survived` 的结果比较：\n",
    "\n",
    "- 决策树模型准确率为 0.6555；\n",
    "- 逻辑回归模型准确率为 0.7536；\n",
    "- 朴素贝叶斯模型准确率为 0.7799。\n",
    "\n",
    "根据准确率的比较，朴素贝叶斯模型表现最好，其次是逻辑回归模型，最后是决策树模型。这表明在这个特定的数据集上，朴素贝叶斯模型能够更好地预测乘客的存活情况。\n",
    "这可能是因为朴素贝叶斯模型能够很好地处理多个特征之间的相关性，适用于该数据集的特征分布。逻辑回归则在处理线性可分问题时表现较好，而决策树模型在该数据集上的表现可能受到过拟合的影响。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
